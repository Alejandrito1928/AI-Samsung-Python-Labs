{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dbe6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['W1', 'B1', 'W2', 'B2', 'W3', 'B3', 'W4', 'B4', 'W5', 'B5'])\n",
      "Dimensiones de la Matriz de Pesos W1: (4, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"relu\"}\n",
    "]\n",
    "\n",
    "def init_layers(nn_architecture):\n",
    "    param_values = {}\n",
    "    # Inicialización de los pesos y sesgos\n",
    "    for layer_idx, layer in enumerate(nn_architecture):\n",
    "        layer_num = layer_idx + 1\n",
    "        # Obtener dimensiones de entrada y salida\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        # Inicializar pesos y sesgos con valores aleatorios pequeños\n",
    "        param_values['W' + str(layer_num)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        # Inicializar sesgos con valores aleatorios pequeños\n",
    "        param_values['B' + str(layer_num)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "            # Almacenar en el diccionario\n",
    "    return param_values\n",
    "    # Retornar los parámetros inicializados\n",
    "#esto muestra como usar la función\n",
    "parametros = init_layers(nn_architecture)\n",
    "\n",
    "print(parametros.keys())\n",
    "print(f\"Dimensiones de la Matriz de Pesos W1: {parametros['W1'].shape}\")\n",
    "\n",
    "#mostramos\n",
    "# EJECUTAR EL CÓDIGO\n",
    "parametros = init_layers(nn_architecture)\n",
    "\n",
    "# definir relu y sigmoid\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c0c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivada de zignoid\n",
    "# definir relu y sigmoid\n",
    "#la funcion relu es la funcion de activacion mas usada en redes neuronales\n",
    "#sirve para introducir no linealidad en el modelo y ayudar a la red a aprender patrones complejos\n",
    "#ademas ayuda a mitigar el problema del gradiente desvanecido que puede ocurrir con otras funciones de activacion como la sigmoide o la tangente hiperbólica\n",
    "#la funcion relu es computacionalmente eficiente ya que solo requiere una operacion de umbralizacion\n",
    "#esto la hace adecuada para redes neuronales profundas y grandes conjuntos de datos\n",
    "\n",
    "#todo esto es vectores y matrices\n",
    "#vecto ejemplo z = np.array([-2, 3, -1,4])\n",
    "#dz = (-2,3,0,0)\n",
    "def relu(Z):\n",
    "    #si es -2 devuelve 0, si es 3 devuelve 3\n",
    "    #si es 0 devuelve 0\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relubackward(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)  # convertir dA a un array de numpy\n",
    "    dZ[Z <= 0] = 0  # cuando Z es menor o igual a 0, establecer dZ en 0\n",
    "    return dZ\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return (Z > 0).astype(float)\n",
    "    #la derivada de relu es 1 para valores positivos y 0 para valores negativos o cero\n",
    "    #esto significa que durante la retropropagacion, los gradientes se propagan solo a \n",
    "    #traves de las neuronas que estan activas (valores positivos)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "#la funcion sigmoide es otra funcion de activacion comunmente utilizada en redes neuronales\n",
    "#convierte cualquier valor real en un valor entre 0 y 1\n",
    "#esto la hace especialmente util para problemas de clasificacion binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, B_curr, activation= \"relu\"):\n",
    "    Z_curr = W_curr @ A_prev + B_curr\n",
    "    if activation == \"relu\":\n",
    "        A_curr = relu(Z_curr)\n",
    "    elif activation == \"sigmoid\":\n",
    "        A_curr = sigmoid(Z_curr)\n",
    "    else:\n",
    "        raise Exception(\"No se ha implementado la funcion de activacion\")\n",
    "    return A_curr, Z_curr\n",
    "#la funcion single_layer_forward_propagation realiza la propagacion hacia adelante para una sola capa de la red neuronal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d57084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_propagation(X, param_values, nn_architecture):\n",
    "    memory = {}\n",
    "    A_curr = X\n",
    "    for layer_idx, layer in enumerate(nn_architecture):\n",
    "        layer_num = layer_idx + 1\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        activation_function_curr = layer[\"activation\"]\n",
    "        W_curr = param_values['W' + str(layer_num)]\n",
    "        B_curr = param_values['B' + str(layer_num)]\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(\n",
    "            A_prev, W_curr, B_curr, activation_function_curr\n",
    "        )\n",
    "\n",
    "        memory['A' + str(layer_num - 1)] = A_prev\n",
    "        memory['Z' + str(layer_num)] = Z_curr\n",
    "    return A_curr, memory\n",
    "#la funcion full_forward_propagation realiza la propagacion hacia adelante a traves de todas las capas de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e308e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prob_into_class(probs):\n",
    "    probs_ = probs.copy()\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "    # Convertir probabilidades en clases binarias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45979cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m data_list \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Creamos la MATRIZ X, transponiéndola para que quede en la forma (2 filas x 6 columnas)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(data_list)\u001b[38;5;241m.\u001b[39mT \n\u001b[0;32m      7\u001b[0m param_values \u001b[38;5;241m=\u001b[39m init_layers(nn_architecture) \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Llama a la función con la MATRIZ X COMPLETA\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# --- PARTE 1: CORRECCIÓN DE LA ESTRUCTURA DE DATOS ---\n",
    "# Definimos los 6 puntos/muestras de entrada (cada lista interna es una muestra)\n",
    "data_list = [[1, 1], [-4, 1], [-1, 4], [-2, 4], [3, -4], [3, -1]]\n",
    "\n",
    "# Creamos la MATRIZ X, transponiéndola para que quede en la forma (2 filas x 6 columnas)\n",
    "X = np.array(data_list).T\n",
    "\n",
    "print(f\"La matriz de entrada X tiene dimensiones (Características x Muestras): {X.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- PARTE 2: EJECUCIÓN DE LA PROPAGACIÓN HACIA ADELANTE ---\n",
    "\n",
    "# Inicializa los parámetros de la red (W y B)\n",
    "param_values = init_layers(nn_architecture)\n",
    "\n",
    "# Ejecuta el Forward Pass completo con la matriz X correcta\n",
    "# A_final es la predicción (Y_hat) y memory guarda los valores intermedios\n",
    "A_final, memory = full_forward_propagation(X, param_values, nn_architecture)\n",
    "\n",
    "print(f\"Salida final de la red (Y_hat): {A_final.shape}\")\n",
    "print(A_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curso_ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
