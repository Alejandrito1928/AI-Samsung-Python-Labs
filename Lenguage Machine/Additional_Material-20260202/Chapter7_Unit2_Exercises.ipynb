{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99e684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import string\n",
    "import string "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75def31",
   "metadata": {},
   "source": [
    "Ejercicio 1: Tokenización básica \\\n",
    "Tokeniza el texto usando separación por espacios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc575e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"El procesamiento de lenguaje natural es fascinante\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c04307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'procesamiento', 'de', 'lenguaje', 'natural', 'es', 'fascinante']\n"
     ]
    }
   ],
   "source": [
    "text = \"El procesamiento de lenguaje natural es fascinante\"\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaccacf",
   "metadata": {},
   "source": [
    "Ejercicio 2: Normalización + tokenización \\\n",
    "Pasa el texto a minúsculas y elimina puntuación antes de tokenizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f8d2ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', 'cómo', 'estás', 'hoy']\n"
     ]
    }
   ],
   "source": [
    "text = \"¡Hola! ¿Cómo estás hoy?\"\n",
    "text = text.lower()\n",
    "chars = string.punctuation + \"¡¿\"\n",
    "text = text.translate(str.maketrans('', '', chars))\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246b835",
   "metadata": {},
   "source": [
    "Ejercicio 3: Tokenización con regex \\\n",
    "Extrae solo palabras usando expresiones regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d93dac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"Python 3.10 es genial, ¿no?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b2ebb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'es', 'genial', 'no']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = re.findall(r'[a-zA-ZáéíóúÁÉÍÓÚñÑ]+', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b87b1e",
   "metadata": {},
   "source": [
    "Ejercicio 4: Palabras y signos como tokens \\\n",
    "Tokeniza conservando puntuación como tokens independientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a6672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hola, ¿cómo estás?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25510a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', ',', '¿', 'cómo', 'estás', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b67ffdd",
   "metadata": {},
   "source": [
    "Ejercicio 5: Tokenización con NLTK \\\n",
    "Usa nltk word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d447a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'is', 'a', 'classic', 'library', 'for', 'NLP', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alere\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"NLTK is a classic library for NLP.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3be76",
   "metadata": {},
   "source": [
    "Ejercicio 6: Tokenización básica con word_tokenize \\\n",
    "Dado el siguiente texto: \"¡Hola! Me llamo Ana. Estoy aprendiendo procesamiento de lenguaje natural con Python3.\"\n",
    "- Utiliza word_tokenize para separar el texto en palabras y signos de puntuación.\n",
    "- Imprime la lista de tokens resultante.\n",
    "- Cuenta cuántas palabras (que contengan letras o números) y cuántos signos de puntuación hay usando expresiones regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd1b0ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['¡Hola', '!', 'Me', 'llamo', 'Ana', '.', 'Estoy', 'aprendiendo', 'procesamiento', 'de', 'lenguaje', 'natural', 'con', 'Python3', '.']\n",
      "Cantidad de palabras: 11\n",
      "Cantidad de signos de puntuación: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alere\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"¡Hola! Me llamo Ana. Estoy aprendiendo procesamiento de lenguaje natural con Python3.\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "words = [t for t in tokens if re.fullmatch(r'\\w+', t)]\n",
    "punctuation = [t for t in tokens if re.fullmatch(r'[^\\w\\s]+', t)]\n",
    "\n",
    "print(\"Cantidad de palabras:\", len(words))\n",
    "print(\"Cantidad de signos de puntuación:\", len(punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e362f",
   "metadata": {},
   "source": [
    "Ejercicio 7: Tokenización con WordPunctTokenizer \\\n",
    "Dado el texto: \"Python es genial, ¿verdad? ¡Sí, lo es!\"\n",
    "- Utiliza WordPunctTokenizer para tokenizar el texto.\n",
    "- Compara la tokenización obtenida con word_tokenize.\n",
    "- Cuenta cuántas palabras (letras o números) y cuántos signos de puntuación hay usando expresiones regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ff5e5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['¡Hola', '!', 'Me', 'llamo', 'Ana', '.', 'Estoy', 'aprendiendo', 'procesamiento', 'de', 'lenguaje', 'natural', 'con', 'Python3', '.']\n",
      "Cantidad de palabras: 11\n",
      "Cantidad de signos de puntuación: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alere\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"¡Hola! Me llamo Ana. Estoy aprendiendo procesamiento de lenguaje natural con Python3.\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "words = [t for t in tokens if re.fullmatch(r'\\w+', t)]\n",
    "punctuation = [t for t in tokens if re.fullmatch(r'[^\\w\\s]+', t)]\n",
    "\n",
    "print(\"Cantidad de palabras:\", len(words))\n",
    "print(\"Cantidad de signos de puntuación:\", len(punctuation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f5414",
   "metadata": {},
   "source": [
    "Ejercicio 8: Tokenización con text_to_word_sequence \\\n",
    "Dado el texto: \"NLTK, Keras y Python3 son herramientas poderosas para NLP.\"\n",
    "- Utiliza text_to_word_sequence de Keras para tokenizar el texto.\n",
    "- Convierte todos los tokens a minúsculas automáticamente.\n",
    "- Cuenta cuántas palabras hay usando expresiones regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e2cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe45fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "406f18f3",
   "metadata": {},
   "source": [
    "Ejercicio 9: Filtrar stopwords de un texto \\\n",
    "Dado el texto: \"El aprendizaje automático es una rama de la inteligencia artificial que estudia cómo las máquinas pueden aprender de los datos.\"\n",
    "- Tokeniza el texto usando word_tokenize.\n",
    "- Filtra las stopwords en español usando NLTK.\n",
    "- Imprime la lista de palabras resultante, sin stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "abbca5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"El aprendizaje automático es una rama de la inteligencia artificial que estudia cómo las máquinas pueden aprender de los datos. En este sentido, la inteligencia artificial deja de depender de instrucciones fijas, ya que el aprendizaje automático dota a los sistemas de la capacidad de ajustar sus propios parámetros de forma autónoma a medida que procesan nueva información.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128447db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75a2e04c",
   "metadata": {},
   "source": [
    "Ejercicio 10: Contar frecuencia de palabras sin stopwords \\\n",
    "Usa el mismo texto que en el Ejercicio 9:\n",
    "- Tokeniza y elimina stopwords.\n",
    "- Calcula la frecuencia de cada palabra y muestra las 5 más comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5e0ca032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d45657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1150c978",
   "metadata": {},
   "source": [
    "Ejercicio 11: Comparar tokenización con y sin stopwords \\\n",
    "Dado el texto: \"La minería de datos permite descubrir patrones interesantes y útiles en grandes cantidades de información.\"\n",
    "- Tokeniza el texto usando word_tokenize.\n",
    "- Muestra los tokens con stopwords y sin stopwords.\n",
    "- Analiza cuántos tokens se eliminan al filtrar las stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "75563118",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto2 = \"La minería de datos permite descubrir patrones interesantes y útiles en grandes cantidades de información.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09854d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "545c53ca",
   "metadata": {},
   "source": [
    "Ejercicio 12: Filtrar stopwords y convertir a minúsculas \\\n",
    "Enunciado: Dado cualquier texto, tokeniza y convierte todos los tokens a minúsculas.\n",
    "- Filtra las stopwords en español.\n",
    "- Imprime el resultado final como lista de palabras únicas ordenadas alfabéticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "56250581",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto3 = \"Los algoritmos de aprendizaje profundo están revolucionando la ciencia de datos.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814241a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa7ac380",
   "metadata": {},
   "source": [
    "Ejercicio 13: Stemming básico \\\n",
    "Dado el texto: \"The cats are running faster than the dogs.\"\n",
    "- Tokeniza el texto usando word_tokenize.\n",
    "- Aplica PorterStemmer a cada token.\n",
    "- Imprime la lista de tokens originales y los tokens stemmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22556e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e67ff0ca",
   "metadata": {},
   "source": [
    "Ejercicio 14: Usando el mismo texto que en el Ejercicio 13:\n",
    "- Aplica WordNetLemmatizer a cada token.\n",
    "- Especifica la categoría gramatical cuando sea posible (por ejemplo, pos='v' para verbos).\n",
    "- Compara los resultados con los del Stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce946f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f921c2bb",
   "metadata": {},
   "source": [
    "Ejercicio 15: Comparación en un texto más largo \\\n",
    "Dado el texto: \"The children are playing in the gardens while their mothers were watching them.\"\n",
    "- Tokeniza el texto.\n",
    "- Aplica Stemming y Lemmatization (con categoría gramatical).\n",
    "- Muestra una tabla comparativa que muestre: Token original | Stemming | Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "88ffee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto2 = \"The children are playing in the gardens while their mothers were watching them.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9d176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2fe02e2",
   "metadata": {},
   "source": [
    "Ejercicio 16: Análisis de frecuencia después de Stemming y Lemmatization\n",
    "- Dado un párrafo, aplica tokenización, eliminación de stopwords y luego Stemming y Lemmatization.\n",
    "- Calcula la frecuencia de las palabras obtenidas por cada método.\n",
    "- Compara cuál método produce palabras más “reconocibles” para análisis posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6565f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto3 = \"Running, runs, and ran are different forms of the verb run. Cats, cat, and cats are animals.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a2766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e518ef7f",
   "metadata": {},
   "source": [
    "Ejercicio 17: Integer Encoding con Keras \\\n",
    "Dado el siguiente conjunto de frases: \\\n",
    "frases = [ \\\n",
    "    \"Me gusta Python\", \\\n",
    "    \"Python es divertido\", \\\n",
    "    \"Me gusta aprender NLP\" \\\n",
    "] \n",
    "- Crea un Tokenizer de Keras y ajusta el texto.\n",
    "- Convierte las frases a secuencias de enteros (Integer Encoding).\n",
    "- Imprime el diccionario de palabras y las secuencias codificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "074322af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "frases = [\n",
    "    \"Me gusta Python\",\n",
    "    \"Python es divertido\",\n",
    "    \"Me gusta aprender NLP\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e944645d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6220d94b",
   "metadata": {},
   "source": [
    "Ejercicio 18: Padding de secuencias\n",
    "Toma las secuencias del Ejercicio 17.\n",
    "- Usa pad_sequences de Keras para que todas las secuencias tengan la misma longitud.\n",
    "- Usa padding ‘pre’ (al inicio) y padding ‘post’ (al final) para ver la diferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c841fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a43e9051",
   "metadata": {},
   "source": [
    "Ejercicio 19: One-Hot Encoding de palabras \\\n",
    "Usa el mismo conjunto de frases.\n",
    "- Convierte las frases a matrices One-Hot usando texts_to_matrix de Keras con mode='binary'.\n",
    "- Imprime el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2efd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78114442",
   "metadata": {},
   "source": [
    "Ejercicio 20: One-Hot Encoding de etiquetas (labels) \\\n",
    "Supongamos que tenemos las siguientes etiquetas para las frases: \\\n",
    "labels = [\"positivo\", \"positivo\", \"neutral\"]\n",
    "- Convierte las etiquetas a Integer Encoding usando un diccionario.\n",
    "- Aplica One-Hot Encoding a las etiquetas.\n",
    "- Imprime las etiquetas codificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc0a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2650d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb669f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d81381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curso_ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
